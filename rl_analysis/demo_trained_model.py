#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Network Echo
"""

import gymnasium as gym
from stable_baselines3 import PPO
from network_echo_env_improved import NetworkEchoEnvImproved
import numpy as np
import time
import os

def demo_trained_model(model_path="ppo_quick_test.zip", episodes=3):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    print("ü§ñ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    print("=" * 50)
    
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for file in os.listdir("."):
            if file.endswith(".zip"):
                print(f"  ‚Ä¢ {file}")
        return
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    config = {
        'mode': 'full',
        'stage': 0,
        'reduce_randomness': True,
        'improved_rewards': True,
        'curriculum_learning': True
    }
    
    env = NetworkEchoEnvImproved(config=config)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
    model = PPO.load(model_path)
    model.set_env(env)
    
    print(f"üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è {episodes} —ç–ø–∏–∑–æ–¥–æ–≤...")
    print()
    
    total_rewards = []
    total_steps = []
    wins = 0
    
    for episode in range(episodes):
        print(f"üéÆ –≠–ø–∏–∑–æ–¥ {episode + 1}/{episodes}")
        print("-" * 30)
        
        obs, info = env.reset()
        step = 0
        total_reward = 0
        
        print(f"üìä –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:")
        print(f"  ‚Ä¢ DP: {obs[0]:.2f} | CPU: {obs[1]:.2f}")
        print(f"  ‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º–º—ã: [{obs[7]:.0f}, {obs[8]:.0f}, {obs[9]:.0f}, {obs[10]:.0f}, {obs[11]:.0f}]")
        print(f"  ‚Ä¢ –£—Ä–æ–≤–Ω–∏: [{obs[2]:.0f}, {obs[3]:.0f}, {obs[4]:.0f}, {obs[5]:.0f}, {obs[6]:.0f}]")
        print(f"  ‚Ä¢ –í—Ä–∞–≥–∏: {obs[14]:.0f} | –£–∑–ª—ã: {obs[15]:.0f}/{obs[16]:.0f}")
        print()
        
        while True:
            # –ü–æ–ª—É—á–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –æ—Ç –º–æ–¥–µ–ª–∏
            action, _ = model.predict(obs, deterministic=True)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            step += 1
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 —à–∞–≥–æ–≤
            if step % 20 == 0:
                print(f"  –®–∞–≥ {step}: DP={obs[0]:.1f}, CPU={obs[1]:.1f}, –ü—Ä–æ–≥—Ä–∞–º–º—ã=[{obs[7]:.0f},{obs[8]:.0f},{obs[9]:.0f},{obs[10]:.0f},{obs[11]:.0f}], –ù–∞–≥—Ä–∞–¥–∞={total_reward:.1f}")
            
            if terminated or truncated:
                break
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–∏–∑–æ–¥–∞
        total_rewards.append(total_reward)
        total_steps.append(step)
        
        if info.get('win', False):
            wins += 1
            result = "‚úÖ –ü–û–ë–ï–î–ê"
        else:
            result = "‚ùå –ü–û–†–ê–ñ–ï–ù–ò–ï"
        
        print(f"üèÅ –≠–ø–∏–∑–æ–¥ {episode + 1} –∑–∞–≤–µ—Ä—à–µ–Ω: {result}")
        print(f"  ‚Ä¢ –®–∞–≥–æ–≤: {step}")
        print(f"  ‚Ä¢ –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.3f}")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward/max(1, step):.3f}")
        print(f"  ‚Ä¢ –ü—Ä–∏—á–∏–Ω–∞: {info.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        print()
    
    env.close()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 30)
    print(f"  ‚Ä¢ –≠–ø–∏–∑–æ–¥–æ–≤: {episodes}")
    print(f"  ‚Ä¢ –ü–æ–±–µ–¥: {wins} ({wins/episodes*100:.1f}%)")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(total_rewards):.3f}")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {np.mean(total_steps):.1f}")
    print(f"  ‚Ä¢ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {max(total_rewards):.3f}")
    print(f"  ‚Ä¢ –•—É–¥—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {min(total_rewards):.3f}")

def demo_with_manual_control(model_path="ppo_quick_test.zip"):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ä—É—á–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
    
    print("üéÆ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –° –†–£–ß–ù–´–ú –£–ü–†–ê–í–õ–ï–ù–ò–ï–ú")
    print("=" * 50)
    
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    config = {
        'mode': 'full',
        'stage': 0,
        'reduce_randomness': True,
        'improved_rewards': True,
        'curriculum_learning': True
    }
    
    env = NetworkEchoEnvImproved(config=config)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_path}")
    model = PPO.load(model_path)
    model.set_env(env)
    
    print("üéØ –†–µ–∂–∏–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
    print("  ‚Ä¢ 'a' - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª—å—é")
    print("  ‚Ä¢ 'm' - –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
    print("  ‚Ä¢ 'q' - –í—ã—Ö–æ–¥")
    print()
    
    obs, info = env.reset()
    step = 0
    total_reward = 0
    
    while True:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        print(f"\nüîÑ –®–∞–≥ {step + 1}")
        print(f"üí∞ DP: {obs[0]:.2f} | üñ•Ô∏è CPU: {obs[1]:.2f} | üéØ –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {obs[12]:.2f}")
        print(f"üì¶ –ü—Ä–æ–≥—Ä–∞–º–º—ã: [{obs[7]:.0f}, {obs[8]:.0f}, {obs[9]:.0f}, {obs[10]:.0f}, {obs[11]:.0f}]")
        print(f"‚≠ê –£—Ä–æ–≤–Ω–∏: [{obs[2]:.0f}, {obs[3]:.0f}, {obs[4]:.0f}, {obs[5]:.0f}, {obs[6]:.0f}]")
        print(f"üëæ –í—Ä–∞–≥–∏: {obs[14]:.0f} | üéØ –£–∑–ª—ã: {obs[15]:.0f}/{obs[16]:.0f}")
        
        # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ä–µ–∂–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        mode = input("\nüéÆ –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º (a/m/q): ").strip().lower()
        
        if mode == 'q':
            break
        elif mode == 'a':
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            print("ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ...")
            action, _ = model.predict(obs, deterministic=True)
            print(f"  –î–µ–π—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏: {action}")
        elif mode == 'm':
            # –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            print("üéÆ –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:")
            print("  0: –ù–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è | 1-5: –ö—É–ø–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É | 6-10: –ê–ø–≥—Ä–µ–π–¥ | 11: –ê—Ç–∞–∫–∞ | 12: –ó–∞—Ö–≤–∞—Ç —É–∑–ª–∞ | 13: –ó–∞—Ö–≤–∞—Ç —Å–µ—Ç–∏")
            try:
                action = int(input("–í–≤–µ–¥–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (0-13): "))
                if action < 0 or action > 13:
                    print("‚ùå –ù–µ–≤–µ—Ä–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤–∏–µ 0")
                    action = 0
            except ValueError:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤–≤–æ–¥! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤–∏–µ 0")
                action = 0
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
            action, _ = model.predict(obs, deterministic=True)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        step += 1
        
        print(f"‚úÖ –î–µ–π—Å—Ç–≤–∏–µ {action} –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")
        print(f"üéÅ –ù–∞–≥—Ä–∞–¥–∞: {reward:.3f}")
        print(f"üìà –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.3f}")
        
        if terminated:
            print(f"\nüèÅ –ò–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ü—Ä–∏—á–∏–Ω–∞: {info.get('reason', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            if info.get('win', False):
                print("üéâ –ü–û–ë–ï–î–ê!")
            else:
                print("üíÄ –ü–æ—Ä–∞–∂–µ–Ω–∏–µ")
            break
            
        if truncated:
            print(f"\n‚è∞ –ò–≥—Ä–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è)")
            break
    
    env.close()
    
    print(f"\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  ‚Ä¢ –®–∞–≥–æ–≤: {step}")
    print(f"  ‚Ä¢ –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.3f}")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —à–∞–≥: {total_reward/max(1, step):.3f}")

def main():
    """–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    print("ü§ñ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    print("=" * 50)
    print("1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è (3 —ç–ø–∏–∑–æ–¥–∞)")
    print("2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("3. –í—ã—Ö–æ–¥")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    available_models = [f for f in os.listdir(".") if f.endswith(".zip")]
    if available_models:
        print("üìÅ –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
        for i, model in enumerate(available_models):
            print(f"  {i+1}. {model}")
        print()
        
        if len(available_models) == 1:
            model_path = available_models[0]
        else:
            try:
                choice = int(input("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (–Ω–æ–º–µ—Ä): ")) - 1
                if 0 <= choice < len(available_models):
                    model_path = available_models[choice]
                else:
                    model_path = available_models[0]
            except:
                model_path = available_models[0]
    else:
        print("‚ùå –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å.")
        return
    
    while True:
        choice = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º (1-3): ").strip()
        
        if choice == '1':
            demo_trained_model(model_path)
            break
        elif choice == '2':
            demo_with_manual_control(model_path)
            break
        elif choice == '3':
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 1-3")

if __name__ == "__main__":
    main() 