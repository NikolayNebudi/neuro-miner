#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å–µ—Ö –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
"""

import os
import csv
import uuid
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from network_echo_env_improved import NetworkEchoEnvImproved
import numpy as np
from datetime import datetime

# –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´
EPISODES_PER_STAGE = 500000  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100k –¥–æ 500k
LEARNING_RATE = 1e-4         # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 3e-4 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
N_STEPS = 4096              # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
BATCH_SIZE = 128            # –£–≤–µ–ª–∏—á–µ–Ω–æ
N_EPOCHS = 8                # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
ENT_COEF = 0.01             # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ exploration

LOG_PATH = os.path.join(os.path.dirname(__file__), 'improved_analysis_log.csv')
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'ppo_improved_final.zip')

class EarlyStoppingCallback(BaseCallback):
    """Callback –¥–ª—è early stopping –ø—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–µ—Ä–∏—è—Ö –ø–æ—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, patience=100, min_episodes=200, verbose=1):
        super().__init__(verbose)
        self.patience = patience
        self.min_episodes = min_episodes
        self.loss_streak = 0
        self.best_reward = -np.inf
        self.no_improvement_count = 0
        self.episode_count = 0
        
    def _on_step(self) -> bool:
        self.episode_count += 1
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö
        if hasattr(self.training_env, 'get_attr'):
            try:
                recent_rewards = []
                for env in self.training_env.envs:
                    if hasattr(env, 'episode_rewards'):
                        recent_rewards.extend(env.episode_rewards[-10:])
                
                if recent_rewards:
                    avg_reward = np.mean(recent_rewards)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
                    if avg_reward > self.best_reward:
                        self.best_reward = avg_reward
                        self.no_improvement_count = 0
                    else:
                        self.no_improvement_count += 1
                    
                    # Early stopping —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–∏–∑–æ–¥–æ–≤
                    if (self.episode_count >= self.min_episodes and 
                        self.no_improvement_count >= self.patience):
                        if self.verbose > 0:
                            print(f"Early stopping triggered after {self.patience} episodes without improvement")
                        return False
            except:
                pass
        
        return True

class AdaptiveLearningRateCallback(BaseCallback):
    """Callback –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ learning rate"""
    
    def __init__(self, initial_lr=1e-4, min_lr=1e-6, factor=0.8, patience=50, verbose=1):
        super().__init__(verbose)
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.factor = factor
        self.patience = patience
        self.best_reward = -np.inf
        self.no_improvement_count = 0
        self.current_lr = initial_lr
        
    def _on_step(self) -> bool:
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω—é—é –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–∏–∑–æ–¥—ã
            recent_rewards = []
            for env in self.training_env.envs:
                if hasattr(env, 'episode_rewards'):
                    recent_rewards.extend(env.episode_rewards[-20:])
            
            if recent_rewards:
                avg_reward = np.mean(recent_rewards)
                
                if avg_reward > self.best_reward:
                    self.best_reward = avg_reward
                    self.no_improvement_count = 0
                else:
                    self.no_improvement_count += 1
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ learning rate
                if self.no_improvement_count >= self.patience:
                    self.current_lr = max(self.current_lr * self.factor, self.min_lr)
                    self.model.learning_rate = self.current_lr
                    self.no_improvement_count = 0
                    
                    if self.verbose > 0:
                        print(f"Learning rate reduced to {self.current_lr:.2e}")
        except:
            pass
        
        return True

class RewardShapingCallback(BaseCallback):
    """Callback –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ reward shaping"""
    
    def __init__(self, verbose=1):
        super().__init__(verbose)
        self.resource_bonus = 0.1
        self.efficiency_bonus = 0.2
        self.exploration_bonus = 0.05
        
    def _on_step(self) -> bool:
        # –≠—Ç–æ—Ç callback –±—É–¥–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        return True

class CurriculumLearningCallback(BaseCallback):
    """Callback –¥–ª—è curriculum learning"""
    
    def __init__(self, stage_thresholds=[0.15, 0.25, 0.35], verbose=1):
        super().__init__(verbose)
        self.stage_thresholds = stage_thresholds
        self.current_stage = 0
        self.stage_episodes = 0
        
    def _on_step(self) -> bool:
        self.stage_episodes += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø
        try:
            recent_rewards = []
            for env in self.training_env.envs:
                if hasattr(env, 'episode_rewards'):
                    recent_rewards.extend(env.episode_rewards[-100:])
            
            if len(recent_rewards) > 100:  # –ú–∏–Ω–∏–º—É–º —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                win_rate = np.mean([1 if r > 0 else 0 for r in recent_rewards[-100:]])
                
                if (self.current_stage < len(self.stage_thresholds) and 
                    win_rate >= self.stage_thresholds[self.current_stage]):
                    self.current_stage += 1
                    if self.verbose > 0:
                        print(f"Progressing to stage {self.current_stage} (win rate: {win_rate:.2f})")
        except:
            pass
        
        return True

# –£–ª—É—á—à–µ–Ω–Ω—ã–π CSV –ª–æ–≥–≥–µ—Ä
def log_episode_improved(session_id, stage, win, reason, score, steps, trace, final_stats=None):
    file_exists = os.path.isfile(LOG_PATH)
    with open(LOG_PATH, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow([
                'session_id', 'stage', 'win', 'reason_for_end', 'final_score', 'total_steps', 'final_trace_level',
                'program_counts', 'avg_levels', 'enemies_killed', 'total_dp', 'total_cpu', 'timestamp'
            ])
        row = [session_id, stage, win, reason, score, steps, trace]
        if final_stats:
            row.append(str(final_stats.get('program_counts', {})))
            row.append(str(final_stats.get('avg_levels', {})))
            row.append(final_stats.get('enemies_killed', 0))
            row.append(final_stats.get('total_dp', 0))
            row.append(final_stats.get('total_cpu', 0))
        else:
            row += ['', '', '', '', '']
        row.append(datetime.now().isoformat())
        writer.writerow(row)

def create_improved_env(stage=0, mode='full'):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
    
    config = {
        'mode': mode,
        'stage': stage,
        'reduce_randomness': True,  # –£–º–µ–Ω—å—à–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å
        'improved_rewards': True,   # –í–∫–ª—é—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã
        'curriculum_learning': True # –í–∫–ª—é—á–∞–µ–º curriculum learning
    }
    
    env = NetworkEchoEnvImproved(config=config)
    return env

def train_stage(stage_name, stage_config, model=None):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    print(f"\nüéØ –≠—Ç–∞–ø: {stage_name}")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env = create_improved_env(stage_config['stage'], stage_config['mode'])
    env = DummyVecEnv([lambda: env])
    env = VecMonitor(env, LOG_PATH)
    
    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if model is None:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=LEARNING_RATE,
            n_steps=N_STEPS,
            batch_size=BATCH_SIZE,
            n_epochs=N_EPOCHS,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            normalize_advantage=True,
            ent_coef=ENT_COEF,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ exploration
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            target_kl=None,
            tensorboard_log="./tensorboard_logs/",
            verbose=1
        )
    else:
        model.set_env(env)
    
    # Callbacks
    callbacks = [
        EarlyStoppingCallback(patience=100, verbose=1),
        AdaptiveLearningRateCallback(initial_lr=LEARNING_RATE, verbose=1),
        CurriculumLearningCallback(verbose=1),
        CheckpointCallback(
            save_freq=max(10000, EPISODES_PER_STAGE // 10),
            save_path=f"./checkpoints/stage_{stage_config['stage']}/",
            name_prefix="ppo_model"
        )
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ
    try:
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–∞–ø–∞ {stage_name}...")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: episodes={EPISODES_PER_STAGE}, lr={LEARNING_RATE}, batch_size={BATCH_SIZE}")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model.learn(
            total_timesteps=EPISODES_PER_STAGE * 1000,  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
            callback=callbacks,
            progress_bar=False  # –û—Ç–∫–ª—é—á–∞–µ–º progress bar –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ—à–∏–±–æ–∫
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = f"ppo_improved_stage_{stage_config['stage']}.zip"
        model.save(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        return model
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∞–ø–∞ {stage_name}: {e}")
        return model
    finally:
        env.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ RL-–æ–±—É—á–µ–Ω–∏—è")
    print("=" * 60)
    print(f"–£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  - –≠–ø–∏–∑–æ–¥–æ–≤ –Ω–∞ —ç—Ç–∞–ø: {EPISODES_PER_STAGE:,}")
    print(f"  - Learning rate: {LEARNING_RATE}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - N steps: {N_STEPS}")
    print(f"  - Entropy coefficient: {ENT_COEF}")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    os.makedirs("./checkpoints", exist_ok=True)
    
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    stages = [
        {
            "name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)",
            "stage": 0,
            "mode": "economy_tutorial",
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–û–±–æ—Ä–æ–Ω–∞",
            "stage": 1,
            "mode": "defense_tutorial", 
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–ü–æ–ª–Ω–∞—è –∏–≥—Ä–∞",
            "stage": 2,
            "mode": "full",
            "episodes": EPISODES_PER_STAGE
        }
    ]
    
    model = None
    
    for stage_idx, stage_config in enumerate(stages):
        print(f"\nüéØ –≠—Ç–∞–ø {stage_idx + 1}/{len(stages)}: {stage_config['name']}")
        
        # –û–±—É—á–∞–µ–º —ç—Ç–∞–ø
        model = train_stage(stage_config['name'], stage_config, model)
        
        if model is None:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {stage_idx + 1}, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
            break
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    if model is not None:
        model.save(MODEL_PATH)
        print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")
    
    print(f"\nüéâ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LOG_PATH}")
    print(f"ü§ñ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_PATH}")

if __name__ == "__main__":
    main() 