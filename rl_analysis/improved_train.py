#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
"""

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from network_echo_env import NetworkEchoEnv
import numpy as np
import pandas as pd
import uuid
import time
from datetime import datetime

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
EPISODES_PER_STAGE = 500000  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 100k –¥–æ 500k
LEARNING_RATE = 1e-4  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
N_STEPS = 4096  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
BATCH_SIZE = 128  # –£–≤–µ–ª–∏—á–µ–Ω–æ
N_EPOCHS = 8  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ

class EarlyStoppingCallback(BaseCallback):
    """Callback –¥–ª—è early stopping –ø—Ä–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–µ—Ä–∏—è—Ö –ø–æ—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, patience=50, min_episodes=100, verbose=1):
        super().__init__(verbose)
        self.patience = patience
        self.min_episodes = min_episodes
        self.loss_streak = 0
        self.best_reward = -np.inf
        self.no_improvement_count = 0
        
    def _on_step(self) -> bool:
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö
        if len(self.training_env.buf_rews) > 0:
            recent_rewards = self.training_env.buf_rews[-1]
            if len(recent_rewards) > 0:
                avg_reward = np.mean(recent_rewards)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
                if avg_reward > self.best_reward:
                    self.best_reward = avg_reward
                    self.no_improvement_count = 0
                else:
                    self.no_improvement_count += 1
                
                # Early stopping
                if self.no_improvement_count >= self.patience:
                    if self.verbose > 0:
                        print(f"Early stopping triggered after {self.patience} episodes without improvement")
                    return False
        
        return True

class AdaptiveLearningRateCallback(BaseCallback):
    """Callback –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ learning rate"""
    
    def __init__(self, initial_lr=1e-4, min_lr=1e-6, factor=0.8, patience=20, verbose=1):
        super().__init__(verbose)
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.factor = factor
        self.patience = patience
        self.best_reward = -np.inf
        self.no_improvement_count = 0
        self.current_lr = initial_lr
        
    def _on_step(self) -> bool:
        if len(self.training_env.buf_rews) > 0:
            recent_rewards = self.training_env.buf_rews[-1]
            if len(recent_rewards) > 0:
                avg_reward = np.mean(recent_rewards)
                
                if avg_reward > self.best_reward:
                    self.best_reward = avg_reward
                    self.no_improvement_count = 0
                else:
                    self.no_improvement_count += 1
                
                # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ learning rate
                if self.no_improvement_count >= self.patience:
                    self.current_lr = max(self.current_lr * self.factor, self.min_lr)
                    self.model.learning_rate = self.current_lr
                    self.no_improvement_count = 0
                    
                    if self.verbose > 0:
                        print(f"Learning rate reduced to {self.current_lr:.2e}")
        
        return True

class CurriculumLearningCallback(BaseCallback):
    """Callback –¥–ª—è curriculum learning"""
    
    def __init__(self, stage_thresholds=[0.1, 0.3, 0.5], verbose=1):
        super().__init__(verbose)
        self.stage_thresholds = stage_thresholds
        self.current_stage = 0
        self.stage_episodes = 0
        
    def _on_step(self) -> bool:
        self.stage_episodes += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø
        if len(self.training_env.buf_rews) > 0:
            recent_rewards = self.training_env.buf_rews[-1]
            if len(recent_rewards) > 100:  # –ú–∏–Ω–∏–º—É–º —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                win_rate = np.mean([1 if r > 0 else 0 for r in recent_rewards[-100:]])
                
                if (self.current_stage < len(self.stage_thresholds) and 
                    win_rate >= self.stage_thresholds[self.current_stage]):
                    self.current_stage += 1
                    if self.verbose > 0:
                        print(f"Progressing to stage {self.current_stage} (win rate: {win_rate:.2f})")
        
        return True

def create_improved_env(stage=0):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
    
    env = NetworkEchoEnv()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
    if hasattr(env, 'set_stage'):
        env.set_stage(stage)
    
    # –£–º–µ–Ω—å—à–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏
    if hasattr(env, 'reduce_randomness'):
        env.reduce_randomness()
    
    return env

def train_improved_model():
    """–û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    print("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    log_file = f"improved_training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    # Callbacks
    callbacks = [
        EarlyStoppingCallback(patience=100, verbose=1),
        AdaptiveLearningRateCallback(initial_lr=LEARNING_RATE, verbose=1),
        CurriculumLearningCallback(verbose=1)
    ]
    
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    stages = [
        {"name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)", "episodes": EPISODES_PER_STAGE, "stage": 0},
        {"name": "–û–±–æ—Ä–æ–Ω–∞", "episodes": EPISODES_PER_STAGE, "stage": 1},
        {"name": "–ü–æ–ª–Ω–∞—è –∏–≥—Ä–∞", "episodes": EPISODES_PER_STAGE, "stage": 2}
    ]
    
    for stage_idx, stage_config in enumerate(stages):
        print(f"\nüéØ –≠—Ç–∞–ø {stage_idx + 1}: {stage_config['name']}")
        print("-" * 40)
        
        # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è —ç—Ç–∞–ø–∞
        env = create_improved_env(stage_config['stage'])
        env = DummyVecEnv([lambda: env])
        env = VecMonitor(env, log_file)
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=LEARNING_RATE,
            n_steps=N_STEPS,
            batch_size=BATCH_SIZE,
            n_epochs=N_EPOCHS,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            normalize_advantage=True,
            ent_coef=0.01,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ exploration
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=True,  # State-dependent exploration
            sde_sample_freq=4,
            target_kl=None,
            tensorboard_log="./tensorboard_logs/",
            verbose=1
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        try:
            model.learn(
                total_timesteps=stage_config['episodes'] * 1000,  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
                callback=callbacks,
                progress_bar=True
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            model_path = f"ppo_improved_stage_{stage_idx}.zip"
            model.save(model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∞–ø–∞ {stage_idx + 1}: {e}")
            continue
        
        finally:
            env.close()
    
    print(f"\n‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_file}")

def analyze_improvements():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"""
    
    print("\nüìä –ê–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏–π:")
    print("=" * 30)
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    try:
        old_df = pd.read_csv('analysis_log.csv', header=None, names=[
            'episode_id', 'stage', 'result', 'reward', 'steps', 'trace',
            'program_counts', 'avg_levels', 'enemies_killed', 'total_dp', 'total_cpu'
        ])
        
        new_df = pd.read_csv('improved_training_log_*.csv', header=None, names=[
            'episode_id', 'stage', 'result', 'reward', 'steps', 'trace',
            'program_counts', 'avg_levels', 'enemies_killed', 'total_dp', 'total_cpu'
        ])
        
        print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"–°—Ç–∞—Ä–∞—è —Å–∏—Å—Ç–µ–º–∞: {len(old_df)} —ç–ø–∏–∑–æ–¥–æ–≤, {len(old_df[old_df['result'] == 'win'])} –ø–æ–±–µ–¥")
        print(f"–ù–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: {len(new_df)} —ç–ø–∏–∑–æ–¥–æ–≤, {len(new_df[new_df['result'] == 'win'])} –ø–æ–±–µ–¥")
        
    except FileNotFoundError:
        print("–§–∞–π–ª—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

if __name__ == "__main__":
    train_improved_model()
    analyze_improvements() 