#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
"""

import json
import os
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
from datetime import datetime

def analyze_training_data():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"""
    print("üìä –ê–ù–ê–õ–ò–ó –°–¢–ê–¢–ò–°–¢–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    
    # –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
    log_file = "actions_log.jsonl"
    if os.path.exists(log_file):
        analyze_action_logs(log_file)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
    results_files = [
        "improved_analysis_log.csv",
        "quick_test_log.csv", 
        "optimized_analysis_log.csv"
    ]
    
    for file in results_files:
        if os.path.exists(file):
            analyze_results_file(file)
    
    # –ê–Ω–∞–ª–∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    analyze_checkpoints()
    
    # –ê–Ω–∞–ª–∏–∑ TensorBoard –ª–æ–≥–æ–≤
    analyze_tensorboard_logs()

def analyze_action_logs(log_file):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ –¥–µ–π—Å—Ç–≤–∏–π"""
    print(f"\nüéÆ –ê–ù–ê–õ–ò–ó –õ–û–ì–û–í –î–ï–ô–°–¢–í–ò–ô: {log_file}")
    print("-" * 40)
    
    file_size = os.path.getsize(log_file) / (1024 * 1024 * 1024)  # GB
    print(f"üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.2f} GB")
    
    # –ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    episodes = set()
    actions = Counter()
    steps_per_episode = defaultdict(int)
    rewards = []
    
    print("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏...")
    with open(log_file, 'r') as f:
        for i, line in enumerate(f):
            if i % 10000 == 0:
                print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i:,} —Å—Ç—Ä–æ–∫...")
            
            try:
                data = json.loads(line)
                episode = data.get('episode', 0)
                step = data.get('step', 0)
                action = data.get('chosen_action', {}).get('action', 'unknown')
                reward = data.get('reward', 0)
                
                episodes.add(episode)
                actions[action] += 1
                steps_per_episode[episode] = max(steps_per_episode[episode], step)
                rewards.append(reward)
                
            except:
                continue
    
    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø:")
    print(f"  üéÆ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(episodes)}")
    print(f"  üìù –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π: {sum(actions.values()):,}")
    print(f"  üìä –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —ç–ø–∏–∑–æ–¥–∞: {np.mean(list(steps_per_episode.values())):.0f} —à–∞–≥–æ–≤")
    print(f"  üéØ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {np.mean(rewards):.3f}")
    print(f"  üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.max(rewards):.3f}")
    print(f"  üìâ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {np.min(rewards):.3f}")
    
    print(f"\nüéØ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–ï–ô–°–¢–í–ò–ô:")
    for action, count in actions.most_common(10):
        percentage = count / sum(actions.values()) * 100
        print(f"  {action}: {count:,} ({percentage:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    print(f"\nüìà –ê–ù–ê–õ–ò–ó –ü–†–û–ì–†–ï–°–°–ê:")
    episode_rewards = defaultdict(list)
    with open(log_file, 'r') as f:
        for line in f:
            try:
                data = json.loads(line)
                episode = data.get('episode', 0)
                reward = data.get('reward', 0)
                episode_rewards[episode].append(reward)
            except:
                continue
    
    if episode_rewards:
        avg_rewards_per_episode = {ep: np.mean(rewards) for ep, rewards in episode_rewards.items()}
        episodes_sorted = sorted(avg_rewards_per_episode.keys())
        rewards_sorted = [avg_rewards_per_episode[ep] for ep in episodes_sorted]
        
        print(f"  üìä –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –ø–æ —ç–ø–∏–∑–æ–¥–∞–º:")
        print(f"    –ü–µ—Ä–≤—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {np.mean(rewards_sorted[:10]):.3f}")
        print(f"    –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤: {np.mean(rewards_sorted[-10:]):.3f}")
        print(f"    –ü—Ä–æ–≥—Ä–µ—Å—Å: {np.mean(rewards_sorted[-10:]) - np.mean(rewards_sorted[:10]):.3f}")

def analyze_results_file(file_path):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    print(f"\nüìã –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í: {file_path}")
    print("-" * 40)
    
    try:
        df = pd.read_csv(file_path)
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
        
        if 'win' in df.columns:
            win_rate = df['win'].mean() * 100
            print(f"üéØ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥: {win_rate:.1f}%")
        
        if 'final_score' in df.columns:
            print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∫–æ–≤:")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ: {df['final_score'].mean():.1f}")
            print(f"  –ú–∞–∫—Å–∏–º—É–º: {df['final_score'].max():.1f}")
            print(f"  –ú–∏–Ω–∏–º—É–º: {df['final_score'].min():.1f}")
        
        if 'total_steps' in df.columns:
            print(f"‚è±Ô∏è –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —à–∞–≥–æ–≤:")
            print(f"  –°—Ä–µ–¥–Ω–µ–µ: {df['total_steps'].mean():.0f}")
            print(f"  –ú–∞–∫—Å–∏–º—É–º: {df['total_steps'].max():.0f}")
            print(f"  –ú–∏–Ω–∏–º—É–º: {df['total_steps'].min():.0f}")
        
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥ –æ–±—É—á–µ–Ω–∏—è: {df.iloc[0]['timestamp']} - {df.iloc[-1]['timestamp']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {file_path}: {e}")

def analyze_checkpoints():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π"""
    print(f"\nüíæ –ê–ù–ê–õ–ò–ó –ß–ï–ö–ü–û–ò–ù–¢–û–í")
    print("-" * 40)
    
    checkpoint_dirs = [
        "checkpoints",
        "models", 
        "best_models"
    ]
    
    for dir_name in checkpoint_dirs:
        if os.path.exists(dir_name):
            print(f"\nüìÅ –ü–∞–ø–∫–∞: {dir_name}")
            files = os.listdir(dir_name)
            model_files = [f for f in files if f.endswith('.zip')]
            
            if model_files:
                print(f"  üì¶ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}")
                for model in model_files:
                    file_path = os.path.join(dir_name, model)
                    size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                    print(f"    {model}: {size:.1f} MB")
            else:
                print(f"  üì≠ –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

def analyze_tensorboard_logs():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ TensorBoard"""
    print(f"\nüìä –ê–ù–ê–õ–ò–ó TENSORBOARD –õ–û–ì–û–í")
    print("-" * 40)
    
    tensorboard_dir = "tensorboard_logs"
    if os.path.exists(tensorboard_dir):
        runs = os.listdir(tensorboard_dir)
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø—É—Å–∫–æ–≤: {len(runs)}")
        
        for run in runs:
            run_path = os.path.join(tensorboard_dir, run)
            if os.path.isdir(run_path):
                files = os.listdir(run_path)
                event_files = [f for f in files if f.startswith('events')]
                if event_files:
                    print(f"  üéØ {run}: {len(event_files)} —Ñ–∞–π–ª–æ–≤ —Å–æ–±—ã—Ç–∏–π")
    else:
        print("üì≠ –ü–∞–ø–∫–∞ TensorBoard –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

def generate_training_report():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
    print(f"\nüìã –û–¢–ß–ï–¢ –û–ë –û–ë–£–ß–ï–ù–ò–ò")
    print("=" * 60)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  üéÆ –°–∏—Å—Ç–µ–º–∞: RL-–æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∏–≥—Ä—ã Network Echo")
    print(f"  üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: PPO —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏")
    print(f"  üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è
    print(f"\n‚ö†Ô∏è –í–´–Ø–í–õ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:")
    print(f"  üî¥ –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤: 34.49 GB (497,539 –∑–∞–ø–∏—Å–µ–π)")
    print(f"  üî¥ –ù–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ")
    print(f"  üî¥ –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è")
    print(f"  üî¥ –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
    
    print(f"\n‚úÖ –†–ï–®–ï–ù–ò–Ø:")
    print(f"  üü¢ –°–æ–∑–¥–∞–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    print(f"  üü¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π (50,000)")
    print(f"  üü¢ –£–º–µ–Ω—å—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π")
    print(f"  üü¢ –£–ª—É—á—à–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print(f"  üìà –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    print(f"  üîß –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥ –≤–∞—à–∏ —Ä–µ—Å—É—Ä—Å—ã")
    print(f"  üìä –†–µ–≥—É–ª—è—Ä–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å")
    print(f"  üíæ –û—á–∏—â–∞—Ç—å —Å—Ç–∞—Ä—ã–µ –ª–æ–≥–∏")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    analyze_training_data()
    generate_training_report()

if __name__ == "__main__":
    main() 