#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ö–∞–Ω–∏–∫ –∏–≥—Ä—ã
"""

import os
import sys
import numpy as np
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
from enhanced_logging_system import GameLogger, EnhancedNetworkEchoEnv

# –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø
EPISODES_PER_STAGE = 500  # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
LEARNING_RATE = 1e-4
N_STEPS = 2048
BATCH_SIZE = 128
N_EPOCHS = 4
ENT_COEF = 0.01

class LoggingCallback(BaseCallback):
    """Callback –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, logger, verbose=1):
        super().__init__(verbose)
        self.logger = logger
        self.episode_count = 0
        self.total_reward = 0
        self.episode_rewards = []
        
    def _on_step(self) -> bool:
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
        if self.n_calls % 100 == 0:
            if self.verbose > 0:
                print(f"–®–∞–≥ {self.n_calls}: –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
        return True
    
    def _on_rollout_end(self) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ rollout"""
        if self.verbose > 0:
            print(f"Rollout –∑–∞–≤–µ—Ä—à–µ–Ω: {self.n_calls} —à–∞–≥–æ–≤")

def create_logging_env(config, logger):
    """–°–æ–∑–¥–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    env = EnhancedNetworkEchoEnv(config=config, logger=logger)
    return env

def train_with_logging(stage_name, stage_config, logger, model=None):
    """–û–±—É—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    print(f"\nüéØ –≠—Ç–∞–ø: {stage_name}")
    print("=" * 50)
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {stage_config}")
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    env = create_logging_env(stage_config, logger)
    env = DummyVecEnv([lambda: env])
    env = VecMonitor(env, f"logs/monitor_{logger.session_id}.csv")
    
    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if model is None:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=LEARNING_RATE,
            n_steps=N_STEPS,
            batch_size=BATCH_SIZE,
            n_epochs=N_EPOCHS,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            normalize_advantage=True,
            ent_coef=ENT_COEF,
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,
            target_kl=None,
            tensorboard_log=f"./tensorboard_logs/PPO_{logger.session_id}/",
            verbose=1
        )
    else:
        model.set_env(env)
    
    # Callbacks
    callbacks = [
        LoggingCallback(logger, verbose=1)
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ
    try:
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–∞–ø–∞ {stage_name}...")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: episodes={EPISODES_PER_STAGE}, lr={LEARNING_RATE}")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model.learn(
            total_timesteps=EPISODES_PER_STAGE * 100,  # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            callback=callbacks,
            progress_bar=False
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = f"ppo_logging_{logger.session_id}_stage_{stage_config.get('stage', 0)}.zip"
        model.save(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        return model
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∞–ø–∞ {stage_name}: {e}")
        return model
    finally:
        env.close()

def test_trained_model(model_path, logger, num_episodes=10):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_path}")
    print("=" * 40)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = PPO.load(model_path)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    test_config = {'mode': 'full', 'stage': 0}
    env = create_logging_env(test_config, logger)
    
    wins = 0
    total_reward = 0
    
    for episode in range(num_episodes):
        print(f"  –¢–µ—Å—Ç —ç–ø–∏–∑–æ–¥ {episode + 1}/{num_episodes}")
        
        obs = env.reset()
        done = False
        step = 0
        episode_reward = 0
        
        while not done and step < 500:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —à–∞–≥–∏
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward
            step += 1
            
            if step % 100 == 0:
                print(f"    –®–∞–≥ {step}: DP={env.state.get('dp', 0)}, CPU={env.state.get('cpu', 0)}")
        
        if episode_reward > 0:
            wins += 1
            print(f"    ‚úÖ –ü–æ–±–µ–¥–∞! –ù–∞–≥—Ä–∞–¥–∞: {episode_reward}")
        else:
            print(f"    ‚ùå –ü–æ—Ä–∞–∂–µ–Ω–∏–µ. –ù–∞–≥—Ä–∞–¥–∞: {episode_reward}")
        
        total_reward += episode_reward
    
    win_rate = wins / num_episodes
    avg_reward = total_reward / num_episodes
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"  –ü–æ–±–µ–¥: {wins}/{num_episodes} ({win_rate:.1%})")
    print(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.2f}")
    
    env.close()
    return win_rate, avg_reward

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –° –î–ï–¢–ê–õ–¨–ù–´–ú –õ–û–ì–ò–†–û–í–ê–ù–ò–ï–ú")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—É—é —Å–µ—Å—Å–∏—é
    session_id = f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger = GameLogger(log_dir="logs", session_id=session_id)
    
    print(f"üìù –°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è: {session_id}")
    print(f"üìÅ –ü–∞–ø–∫–∞ –ª–æ–≥–æ–≤: {logger.log_dir}")
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
    os.makedirs("logs", exist_ok=True)
    os.makedirs("tensorboard_logs", exist_ok=True)
    
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è
    stages = [
        {
            "name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)",
            "stage": 0,
            "mode": "economy_tutorial",
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–û–±–æ—Ä–æ–Ω–∞ (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)",
            "stage": 1,
            "mode": "defense_tutorial",
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–ü–æ–ª–Ω–∞—è –∏–≥—Ä–∞ (—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º)",
            "stage": 2,
            "mode": "full",
            "episodes": EPISODES_PER_STAGE
        }
    ]
    
    model = None
    stage_results = []
    
    for stage_idx, stage_config in enumerate(stages):
        print(f"\nüéØ –≠—Ç–∞–ø {stage_idx + 1}/{len(stages)}: {stage_config['name']}")
        
        # –û–±—É—á–∞–µ–º —ç—Ç–∞–ø
        model = train_with_logging(stage_config['name'], stage_config, logger, model)
        
        if model is None:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {stage_idx + 1}, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
            break
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
        model_path = f"ppo_logging_{logger.session_id}_stage_{stage_config.get('stage', 0)}.zip"
        if os.path.exists(model_path):
            win_rate, avg_reward = test_trained_model(model_path, logger, num_episodes=5)
            stage_results.append({
                'stage': stage_config['name'],
                'win_rate': win_rate,
                'avg_reward': avg_reward
            })
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    
    for result in stage_results:
        print(f"  {result['stage']}:")
        print(f"    –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥: {result['win_rate']:.1%}")
        print(f"    –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {result['avg_reward']:.2f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    if model is not None:
        final_model_path = f"ppo_logging_{logger.session_id}_final.zip"
        model.save(final_model_path)
        print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –ª–æ–≥–æ–≤
    summary = logger.get_session_summary()
    if summary:
        print(f"\nüìà –°–í–û–î–ö–ê –õ–û–ì–û–í:")
        print(f"  –í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {summary.get('total_episodes', 0)}")
        print(f"  –ü–æ–±–µ–¥: {summary.get('wins', 0)}")
        print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥: {summary.get('win_rate', 0):.1%}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {summary.get('avg_score', 0):.2f}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª–æ–≥–∞—Ö
    print(f"\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –õ–û–ì–ò:")
    print(f"  –≠–ø–∏–∑–æ–¥—ã: {logger.episode_log}")
    print(f"  –î–µ–π—Å—Ç–≤–∏—è: {logger.action_log}")
    print(f"  –ö–∞—Ä—Ç—ã: {logger.map_log}")
    print(f"  –ë–∞–ª–∞–Ω—Å: {logger.balance_log}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
    for log_file in [logger.episode_log, logger.action_log, logger.map_log, logger.balance_log]:
        if os.path.exists(log_file):
            size = os.path.getsize(log_file)
            print(f"  {os.path.basename(log_file)}: {size} –±–∞–π—Ç")
    
    print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ö–∞–Ω–∏–∫ –∏–≥—Ä—ã –∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ—Ç–∞.")
    print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è –∏–≥—Ä—ã.")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc() 