#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RL-–æ–±—É—á–µ–Ω–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∞–∑–º–µ—Ä–∞ –ª–æ–≥–æ–≤
"""

import os
import csv
import uuid
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from network_echo_env_optimized import NetworkEchoEnvOptimized
import numpy as np
from datetime import datetime

# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´
EPISODES_PER_STAGE = 50000   # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 500k –¥–æ 50k
LEARNING_RATE = 1e-4
N_STEPS = 2048               # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
BATCH_SIZE = 64              # –£–º–µ–Ω—å—à–µ–Ω–æ
N_EPOCHS = 4                 # –£–º–µ–Ω—å—à–µ–Ω–æ
ENT_COEF = 0.01

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
MAX_LOG_ENTRIES = 50000      # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –ª–æ–≥–∞—Ö
LOG_PATH = os.path.join(os.path.dirname(__file__), 'optimized_analysis_log.csv')
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'ppo_optimized_final.zip')

class OptimizedCallback(BaseCallback):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π callback —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, verbose=1):
        super().__init__(verbose)
        self.episode_count = 0
        self.total_reward = 0
        self.episode_rewards = []
        
    def _on_step(self) -> bool:
        self.episode_count += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
        if self.episode_count % 1000 == 0:
            if self.verbose > 0:
                print(f"–®–∞–≥ {self.episode_count}: –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è...")
        
        return True
    
    def _on_rollout_end(self) -> None:
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ rollout"""
        if self.verbose > 0:
            print(f"Rollout –∑–∞–≤–µ—Ä—à–µ–Ω: {self.n_calls} —à–∞–≥–æ–≤")

class MemoryEfficientCallback(BaseCallback):
    """Callback –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, max_episodes=1000, verbose=1):
        super().__init__(verbose)
        self.max_episodes = max_episodes
        self.episode_count = 0
        
    def _on_step(self) -> bool:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤
        if hasattr(self.training_env, 'get_attr'):
            try:
                episode_count = 0
                for env in self.training_env.envs:
                    if hasattr(env, 'episode_idx'):
                        episode_count = max(episode_count, env.episode_idx)
                
                if episode_count >= self.max_episodes:
                    if self.verbose > 0:
                        print(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —ç–ø–∏–∑–æ–¥–æ–≤ ({self.max_episodes}). –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.")
                    return False
            except:
                pass
        
        return True

def log_episode_optimized(session_id, stage, win, reason, score, steps, trace, final_stats=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–ø–∏–∑–æ–¥–æ–≤"""
    file_exists = os.path.isfile(LOG_PATH)
    with open(LOG_PATH, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow([
                'session_id', 'stage', 'win', 'reason_for_end', 'final_score', 'total_steps', 'final_trace_level',
                'program_counts', 'avg_levels', 'enemies_killed', 'total_dp', 'total_cpu', 'timestamp'
            ])
        row = [session_id, stage, win, reason, score, steps, trace]
        if final_stats:
            row.append(str(final_stats.get('program_counts', {})))
            row.append(str(final_stats.get('avg_levels', {})))
            row.append(final_stats.get('enemies_killed', 0))
            row.append(final_stats.get('total_dp', 0))
            row.append(final_stats.get('total_cpu', 0))
        else:
            row += ['', '', '', '', '']
        row.append(datetime.now().isoformat())
        writer.writerow(row)

def create_optimized_env(stage=0, mode='full'):
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ª–æ–≥–æ–≤"""
    config = {
        'mode': mode,
        'stage': stage,
        'reduce_randomness': True,
        'improved_rewards': True,
        'curriculum_learning': True
    }
    env = NetworkEchoEnvOptimized(
        config=config, 
        log_actions=True, 
        log_path="optimized_actions_log.jsonl",
        max_log_entries=MAX_LOG_ENTRIES
    )
    return env

def train_stage_optimized(stage_name, stage_config, model=None):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    print(f"\nüéØ –≠—Ç–∞–ø: {stage_name}")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env = create_optimized_env(stage_config['stage'], stage_config['mode'])
    env = DummyVecEnv([lambda: env])
    env = VecMonitor(env, LOG_PATH)
    
    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if model is None:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=LEARNING_RATE,
            n_steps=N_STEPS,
            batch_size=BATCH_SIZE,
            n_epochs=N_EPOCHS,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            normalize_advantage=True,
            ent_coef=ENT_COEF,
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,
            target_kl=None,
            tensorboard_log="./tensorboard_logs/",
            verbose=1
        )
    else:
        model.set_env(env)
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ callbacks
    callbacks = [
        OptimizedCallback(verbose=1),
        MemoryEfficientCallback(max_episodes=EPISODES_PER_STAGE, verbose=1),
        CheckpointCallback(
            save_freq=max(5000, EPISODES_PER_STAGE // 20),
            save_path=f"./checkpoints/stage_{stage_config['stage']}/",
            name_prefix="ppo_optimized"
        )
    ]
    
    # –û–±—É—á–µ–Ω–∏–µ
    try:
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —ç—Ç–∞–ø–∞ {stage_name}...")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: episodes={EPISODES_PER_STAGE}, lr={LEARNING_RATE}, batch_size={BATCH_SIZE}")
        print(f"–ú–∞–∫—Å–∏–º—É–º –ª–æ–≥–æ–≤: {MAX_LOG_ENTRIES}")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model.learn(
            total_timesteps=EPISODES_PER_STAGE * 100,  # –£–º–µ–Ω—å—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
            callback=callbacks,
            progress_bar=False
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = f"ppo_optimized_stage_{stage_config['stage']}.zip"
        model.save(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        return model
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∞–ø–∞ {stage_name}: {e}")
        return model
    finally:
        env.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ RL-–æ–±—É—á–µ–Ω–∏—è")
    print("=" * 60)
    print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  - –≠–ø–∏–∑–æ–¥–æ–≤ –Ω–∞ —ç—Ç–∞–ø: {EPISODES_PER_STAGE:,}")
    print(f"  - Learning rate: {LEARNING_RATE}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - N steps: {N_STEPS}")
    print(f"  - Entropy coefficient: {ENT_COEF}")
    print(f"  - –ú–∞–∫—Å–∏–º—É–º –ª–æ–≥–æ–≤: {MAX_LOG_ENTRIES:,}")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    os.makedirs("./checkpoints", exist_ok=True)
    
    # –≠—Ç–∞–ø—ã –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    stages = [
        {
            "name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)",
            "stage": 0,
            "mode": "economy_tutorial",
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–û–±–æ—Ä–æ–Ω–∞",
            "stage": 1,
            "mode": "defense_tutorial", 
            "episodes": EPISODES_PER_STAGE
        },
        {
            "name": "–ü–æ–ª–Ω–∞—è –∏–≥—Ä–∞",
            "stage": 2,
            "mode": "full",
            "episodes": EPISODES_PER_STAGE
        }
    ]
    
    model = None
    
    for stage_idx, stage_config in enumerate(stages):
        print(f"\nüéØ –≠—Ç–∞–ø {stage_idx + 1}/{len(stages)}: {stage_config['name']}")
        
        # –û–±—É—á–∞–µ–º —ç—Ç–∞–ø
        model = train_stage_optimized(stage_config['name'], stage_config, model)
        
        if model is None:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {stage_idx + 1}, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
            break
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    if model is not None:
        model.save(MODEL_PATH)
        print(f"\n‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")
    
    print(f"\nüéâ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LOG_PATH}")
    print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_PATH}")

if __name__ == "__main__":
    main() 