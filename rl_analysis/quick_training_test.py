#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 8000 —à–∞–≥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
"""

import os
import csv
import uuid
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from network_echo_env_improved import NetworkEchoEnvImproved
import numpy as np
from datetime import datetime

# –ë–´–°–¢–†–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
QUICK_TOTAL_TIMESTEPS = 8000  # –í—Å–µ–≥–æ 8000 —à–∞–≥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
QUICK_LEARNING_RATE = 1e-4
QUICK_N_STEPS = 1024          # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
QUICK_BATCH_SIZE = 64          # –£–º–µ–Ω—å—à–µ–Ω–æ
QUICK_N_EPOCHS = 4             # –£–º–µ–Ω—å—à–µ–Ω–æ
QUICK_ENT_COEF = 0.01

LOG_PATH = os.path.join(os.path.dirname(__file__), 'quick_test_log.csv')
MODEL_PATH = os.path.join(os.path.dirname(__file__), 'ppo_quick_test.zip')

class QuickTrainingCallback(BaseCallback):
    """Callback –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, verbose=1):
        super().__init__(verbose)
        self.step_count = 0
        self.episode_count = 0
        self.reward_history = []
        
    def _on_step(self) -> bool:
        self.step_count += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.step_count % 1000 == 0:
            print(f"üìä –®–∞–≥ {self.step_count}/{QUICK_TOTAL_TIMESTEPS}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–≥—Ä–∞–¥–∞—Ö
            try:
                recent_rewards = []
                for env in self.training_env.envs:
                    if hasattr(env, 'episode_rewards'):
                        recent_rewards.extend(env.episode_rewards[-10:])
                
                if recent_rewards:
                    avg_reward = np.mean(recent_rewards)
                    max_reward = np.max(recent_rewards)
                    min_reward = np.min(recent_rewards)
                    print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.3f}")
                    print(f"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {max_reward:.3f}")
                    print(f"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {min_reward:.3f}")
                    
                    self.reward_history.append({
                        'step': self.step_count,
                        'avg_reward': avg_reward,
                        'max_reward': max_reward,
                        'min_reward': min_reward
                    })
            except Exception as e:
                print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–∞–≥—Ä–∞–¥: {e}")
        
        return True

def create_quick_env():
    """–°–æ–∑–¥–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    config = {
        'mode': 'economy_tutorial',  # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞
        'stage': 0,
        'reduce_randomness': True,
        'improved_rewards': True,
        'curriculum_learning': False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    }
    env = NetworkEchoEnvImproved(config=config, log_actions=True, log_path="quick_test_actions.jsonl")
    return env

def log_quick_test(session_id, step, avg_reward, max_reward, min_reward):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞"""
    file_exists = os.path.isfile(LOG_PATH)
    with open(LOG_PATH, 'a', newline='') as csvfile:
        writer = csv.writer(csvfile)
        if not file_exists:
            writer.writerow(['session_id', 'step', 'avg_reward', 'max_reward', 'min_reward', 'timestamp'])
        row = [session_id, step, avg_reward, max_reward, min_reward, datetime.now().isoformat()]
        writer.writerow(row)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 50)
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞:")
    print(f"  - –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {QUICK_TOTAL_TIMESTEPS:,}")
    print(f"  - Learning rate: {QUICK_LEARNING_RATE}")
    print(f"  - Batch size: {QUICK_BATCH_SIZE}")
    print(f"  - N steps: {QUICK_N_STEPS}")
    print(f"  - N epochs: {QUICK_N_EPOCHS}")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏
    session_id = str(uuid.uuid4())[:8]
    print(f"üÜî ID —Å–µ—Å—Å–∏–∏: {session_id}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è...")
        env = create_quick_env()
        env = DummyVecEnv([lambda: env])
        env = VecMonitor(env, LOG_PATH)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–ª—è VecEnv)
        print("‚úÖ –û–∫—Ä—É–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –±—ã—Å—Ç—Ä—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ PPO...")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=QUICK_LEARNING_RATE,
            n_steps=QUICK_N_STEPS,
            batch_size=QUICK_BATCH_SIZE,
            n_epochs=QUICK_N_EPOCHS,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            clip_range_vf=None,
            normalize_advantage=True,
            ent_coef=QUICK_ENT_COEF,
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,
            target_kl=None,
            tensorboard_log="./tensorboard_logs/",
            verbose=1
        )
        
        # Callbacks
        callbacks = [
            QuickTrainingCallback(verbose=1),
            CheckpointCallback(
                save_freq=2000,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ 2000 —à–∞–≥–æ–≤
                save_path="./checkpoints/quick_test/",
                name_prefix="ppo_quick"
            )
        ]
        
        # –û–±—É—á–µ–Ω–∏–µ
        print(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {QUICK_TOTAL_TIMESTEPS} —à–∞–≥–æ–≤...")
        print("‚è±Ô∏è  –≠—Ç–æ –∑–∞–π–º–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 1-2 –º–∏–Ω—É—Ç—ã...")
        
        model.learn(
            total_timesteps=QUICK_TOTAL_TIMESTEPS,
            callback=callbacks,
            progress_bar=True
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model.save(MODEL_PATH)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        test_env = DummyVecEnv([create_quick_env])
        obs = test_env.reset()
        total_reward = 0
        steps = 0
        max_steps = 1000
        for step in range(max_steps):
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = test_env.step(action)
            # obs, reward, done, info - —ç—Ç–æ –±–∞—Ç—á–∏ (–º–∞—Å—Å–∏–≤—ã)
            total_reward += reward[0]
            steps += 1
            if done[0]:
                break
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
        print(f"   –®–∞–≥–æ–≤: {steps}")
        print(f"   –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.3f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —à–∞–≥: {total_reward/steps:.3f}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        log_quick_test(session_id, QUICK_TOTAL_TIMESTEPS, total_reward/steps, total_reward, total_reward)
        print(f"\nüéâ –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {LOG_PATH}")
        print(f"ü§ñ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {MODEL_PATH}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'env' in locals():
            env.close()

if __name__ == "__main__":
    main() 